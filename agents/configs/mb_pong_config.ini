[RUN]
# directory name to save this experiment in
NAME=MBPong_train_vq
#cpu vs gpu set by argument
DEVICE=cuda
# load buffer from previous agent
REPLAY_MEMORY_LOADPATH =
REPLAY_MEMORY_VALID_LOADPATH = 
# min steps needed to start training neural nets
#MIN_STEPS_TO_LEARN=10000
MIN_STEPS_TO_LEARN=500000
# if blank then default name is used
TRAIN_BUFFER_NAME=
# if blank then default name is used
EVAL_BUFFER_NAME=
# Number of episodes to run
N_EPOCHS = 90000
TRAIN_SEED = 14342
EVAL_SEED = 100
# Buffer size for experience replay
BUFFER_SIZE = 500000
# how often to write pkl of model and npz of data buffer
CHECKPOINT_EVERY_STEPS = 100000
# I think this randomness might need to be higher
# EPS initially is usually high, then tapers down. With bootstrap, this isn't
# needed I think
EPS_INIT = 0.01
# 0.01 in osband
EPS_FINAL = 0.01
# load a random train and valid buffers from a random agent
# if the random buffer doesn't exist, then create one
LOAD_RANDOM_BUFFERS = True

[EVAL]
# How often to take a random step in evaluation.
# 0 in osband, .05 in others....
EPS_EVAL = 0.05
# num examples to average in eval
NUM_EVAL_EPISODES = 1
# how often to run evaluation episodes
EVAL_FREQUENCY = 500000

[ENV]
# PATH to GAME ALE
GAME=roms/pong.bin
# make rewards be sign(reward) from the emulator - this gives us three rewards
# back. For some games like breakout, there is no negative reward, so we have
# unused reward space - is this good?
# For now - i am going to specify these, because we can't always get rewards/actions 
# in random beginning so it is hard to know exactly how many - this allows us to setup the latent model correctly
REWARD_SPACE = [0,1,2]
ACTION_SPACE = [0,1,2,3,4]
# divide the float(of uint) by this number to normalize - max val of data is 255
NORM_BY = 255.
# how many past frames to use for state input
HISTORY_SIZE = 4
# how big are the observations
OBS_WIDTH = 84
OBS_HEIGHT = 84
# number of steps before the env should end
# 50e6 steps is 200e6 frames
MAX_FRAMES = 200000000
# number of frames to skip between each observation
# in general - "frames" are the images given from the emulator and "steps" are the
# observations presented to the agent
# deterministic frame skips to match deepmind
FRAME_SKIP = 4
# Orig dqn give 18k steps, Rainbow seems to give 27k steps
# this is steps according to the agent - so MAX_EPISODE_STEPS*FRAME_SKIP will
# give the number of emulator frames
MAX_EPISODE_STEPS = 27000
# random number of noops applied to beginning of each episode
MAX_NO_OP_FRAMES = 30
# do you send finished=true to agent while training when it loses a life?
# it will train faster this way, but some papers do not do it this way. I don't
# remember which don't atm.
DEAD_AS_END = True

[PLOT]
# min score to plot gif in eval
MIN_SCORE_GIF=-1
# how often to plot in training
PLOT_EVERY_EPISODES = 5
# Used for plotting demarcation - should be -1
RANDOM_HEAD = -1

[SEARCH]
N_PLAYOUT=50

[VQ]
# how often to train the VQ model - 6400 in simple
VQ_LEARN_EVERY_STEPS = 6400
# load from presaved model - if empty - it will start from scratch
VQ_MODEL_LOADPATH =

[DQN]
# SIZE Expected by DQN - should make this more automated
# 10*10*16
RESHAPE_SIZE = 1600
# use dueling dqn
DUELING = True
# use double dqn
DOUBLE_DQN= True
# turn on to use randomized prior
PRIOR = True
# what to scale prior by
PRIOR_SCALE = 10
# number of bootstrap heads to use. when 1, this is a normal dqn
N_ENSEMBLE = 9
# Probability of experience to go to each head - if 1, every experience goes to every head
BERNOULLI_PROBABILITY=0.9
# how often to update target network
# 500000 may be too much
TARGET_UPDATE=10000
# train DQN network every 4 steps after MIN_STEPS_TO_LEARN
LEARN_EVERY_STEPS=4
#6.25e-5
ADAM_LEARNING_RATE = .0000625 
# Batch size to use for learning
BATCH_SIZE = 64
# Gamma weight in Q update
GAMMA = .99
#Gradient clipping setting
CLIP_GRAD = 5
#if use_embedding in ensemble model - input should be float, otherwise long
USE_EMBEDDING = False
[FORWARD]
# TODO - the forward model is not incorporated yet
FORWARD_DROPOUT = 0.25
# 1e-5
FORWARD_LEARNING_RATE = .00001
FORWARD_BATCH_SIZE = 16

